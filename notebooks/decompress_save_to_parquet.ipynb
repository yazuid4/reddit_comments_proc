{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c216356-6835-4f91-a259-3778d8285372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import zstandard as zstd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import from_unixtime, col\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, TimestampType,\n",
    "    StringType, IntegerType, BooleanType, LongType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c708fd03-b422-4c09-b5fb-6332b1400271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_and_parse_file(path):\n",
    "    results = []\n",
    "    try:\n",
    "        dctx = zstd.ZstdDecompressor(max_window_size=2**31)\n",
    "\n",
    "        with open(path, \"rb\") as fh:\n",
    "            with dctx.stream_reader(fh) as reader:\n",
    "                buffer = \"\"\n",
    "                while True:\n",
    "                    # read 128 MB chunk\n",
    "                    chunk = reader.read(2**27)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "\n",
    "                    chunk_str = buffer + chunk.decode(\"utf-8\", errors=\"ignore\")\n",
    "                    lines = chunk_str.split(\"\\n\")\n",
    "                    # keep incomplete line\n",
    "                    buffer = lines[-1]  \n",
    "\n",
    "                    for line in lines[:-1]:\n",
    "                        if line.strip():\n",
    "                            try:\n",
    "                                data = json.loads(line)\n",
    "                                results.append(data)\n",
    "                            except json.JSONDecodeError:\n",
    "                                continue\n",
    "    except Exception as e:\n",
    "        print(f\"[{path}] Decompression error: {e}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e0ce2d-588a-4fb5-814f-fc72d47442b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"controversiality\", IntegerType(), True),\n",
    "    StructField(\"body\", StringType(), True),\n",
    "    StructField(\"subreddit_id\", StringType(), True),\n",
    "    StructField(\"link_id\", StringType(), True),\n",
    "    StructField(\"stickied\", BooleanType(), True),\n",
    "    StructField(\"subreddit\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"ups\", IntegerType(), True),\n",
    "    StructField(\"author_flair_css_class\", StringType(), True),\n",
    "    StructField(\"created_utc\", LongType(), True),\n",
    "    StructField(\"author_flair_text\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"edited\", BooleanType(), True),\n",
    "    StructField(\"parent_id\", StringType(), True),\n",
    "    StructField(\"gilded\", IntegerType(), True),\n",
    "    StructField(\"distinguished\", StringType(), True),\n",
    "    StructField(\"retrieved_on\", LongType(), True),\n",
    "    # StructField(\"year\", StringType(), True),\n",
    "    # StructField(\"month\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2781c9-3cbd-490a-8291-e07dc3b663f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"project-zstd\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96e8c52-8f3b-49c6-8ed8-6746cbf59b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/comments/\"\n",
    "\n",
    "for file in os.listdir(input_path):\n",
    "    # create df\n",
    "    file_path = os.path.join(input_path, file)\n",
    "    decompressed_results = decompress_and_parse_file(file_path)\n",
    "\n",
    "    # check edited type:\n",
    "    decompressed_results = [{**row, \"edited\": row.get(\"edited\") if isinstance(row.get(\"edited\"), bool) else False}\n",
    "                          for row in decompressed_results]\n",
    "\n",
    "    # create dataframe\n",
    "    df = spark.createDataFrame(decompressed_results, schema=schema)\n",
    "    \n",
    "    #  timestamps\n",
    "    df = df\\\n",
    "        .withColumn(\"created_utc_ts\", from_unixtime(col(\"created_utc\")).cast(\"timestamp\"))\\\n",
    "        .withColumn(\"retrieved_on_ts\", from_unixtime(col(\"retrieved_on\")).cast(\"timestamp\"))\\\n",
    "        .drop(\"retrieved_on\")\\\n",
    "        .drop(\"created_utc\")\n",
    "    \n",
    "    # year & month\n",
    "    year  = file.split('-')[0].split('_')[1]\n",
    "    month = file.split('-')[1].split('.')[0]\n",
    "    df = df.withColumn(\"month\", F.lit(month))\\\n",
    "             .withColumn(\"year\", F.lit(year))\n",
    "\n",
    "    df.coalesce(1)\\\n",
    "            .write.parquet(f\"../data/raw/pq/{year}/{month}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a3642-dde2-4091-a73e-68ecf4cfbba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
